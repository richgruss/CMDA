age <- c(1,2,3,4,5,6,7,8)
weight <- c(5,6,7,2,3,4,9,8)
mean(weight)
mean(age)
cor(age,weight)
plot(age,weight)
weight <- c(12,16,19,32,55,100,200,600)
plot(age,weight)
cor(age,weight)
help.start()
cwd
pwd
#load the iris data
data(iris)
#examine the data
names(iris)
head(iris)
summary(iris)
#load the iris data
data(iris)
#examine the data
names(iris)
head(iris)
summary(iris)
#the last column is the species.  take cols 1-4 for features
features <- iris[,1:4]
#variabels are raw lengths, normalize
scaled_features <- scale(features)
#we need the data as a frame to use Shiny app
scaled_features_frame <-as.data.frame(scaled_features)
#fifth column is species name
label_points <- iris[,5]
#double check data
names(scaled_features_frame)
summary(scaled_features)
#for exploration only--see the distances between
distance <- dist(scaled_features, method = "euclidean")
data(iris)
#examine the data
names(iris)
distance_frame <- as.data.frame(distance)
distance
hier_cl <- hclust(distance, method="ward.D")
plot(hier_cl, labels=iris$Species)
rect.hclust(hier_cl, k=3)
hierarchical_clustering_groups <- cutree(hier_cl, k = 3)
print(hierarchical_clustering_groups)
rect.hclust(hier_cl, k=3)
compare <- cbind(hierarchical_clustering_groups,kmeans_clusters$cluster)
compare <- as.data.frame(compare)
names(compare) <- c("Hierarchical", "kmeans")
compare <- cbind(iris$Species,compare)
compare
kmeans_clusters <- kmeans(distance, 3 , nstart=100, iter.max=100)
kmeans_clusters
compare <- cbind(hierarchical_clustering_groups,kmeans_clusters$cluster)
compare <- as.data.frame(compare)
names(compare) <- c("Hierarchical", "kmeans")
compare <- cbind(iris$Species,compare)
compare
shiny::runApp('git/CMDA/lectures/32/App_3')
#load the iris data
data(iris)
#examine the data
names(iris)
head(iris)
summary(iris)
#the last column is the species.  take cols 1-4 for features
features <- iris[,1:4]
#variabels are raw lengths, normalize
scaled_features <- scale(features)
#FOR 10.3 : we need the data as a frame to use Shiny app
scaled_features_frame <-as.data.frame(scaled_features)
#fifth column is species name
label_points <- iris[,5]
#double check data
names(scaled_features_frame)
summary(scaled_features)
#see the distances between each observed iris
#rows and columns are both iris observations, so this gives
#a symmetrical matrix
distance <- dist(scaled_features, method = "euclidean")
distance
####### 10.1: Hierarchical Clustering #############################
hier_cl <- hclust(distance, method="ward.D")
plot(hier_cl, labels=iris$Species)
rect.hclust(hier_cl, k=3)
hierarchical_clustering_groups <- cutree(hier_cl, k = 3)
#gives a vector that maps each of the 150 observations to a cluster
print(hierarchical_clustering_groups) #print dendrogram
######### 10.2 K-Means Clustering ###################################
kmeans_clusters <- kmeans(distance, 3 , nstart=100, iter.max=100)
kmeans_clusters
#Compare the two clustering algo results
compare <- cbind(hierarchical_clustering_groups,kmeans_clusters$cluster)
compare <- as.data.frame(compare)
names(compare) <- c("Hierarchical", "kmeans")
compare <- cbind(iris$Species,compare)
compare
#both alogorithms do well in grouping the setosas and the versicolors together.
#Hierarchial used cluster 1 and kmeans called it 3, but the important point
#is that both algorithms grouped them accuratey. Both were less successful
#with the virginicas, though.  Both alogorithsm were all over the place
#in the groupings of that species.
shiny::runApp('git/CMDA/lectures/32/App_3')
shiny::runApp('git/CMDA/lectures/32/App_3')
#KNN
setwd('/Users/rgruss/git/CMDA/lectures/34')
load('KDD2009.Rdata')
setwd('/Users/rgruss/git/CMDA/lectures/34')
load('KDD2009.Rdata')
names(dTrain)
head(dTrain$churn) # 1 and -1 type variable; 1 is pos outcome
knnTrain <- dTrain[,selVars]
names(knnTrain)
#examine features and classes to know our data
head(response)
head(knnTrain)
dim(knnTrain)
install.packages('class')
library(class)
system.time(knnDecision <- knn(knnTrain,knnTrain,response,k=200,prob=T))
?knn #to learn more about the knn implementation
response <- dTrain$churn > 0
head(response)
system.time(knnDecision <- knn(knnTrain,knnTrain,response,k=200,prob=T))
?knn #to learn more about the knn implementation
head(knnDecision)
knnPred <- ifelse(knnDecision==TRUE,
attributes(knnDecision)$prob,
1-(attributes(knnDecision)$prob))
head(knnPred)
library(ROCR)
eval <- prediction(knnPred, response) #from ROCR package
auc_calc <- performance(eval,'auc')
auc_calc@y.values #special object; this is how we extract the exact AUC part
#compare to logistic regression AUC and system time
f <- paste('response ~ ',paste(selVars,collapse=' + '),sep='') #create formula
system.time(gmodel <- glm(as.formula(f),
data=knnTrain,
family=binomial(link='logit'))) #get system time and train the model
log_predict <- predict(gmodel,
newdata=knnTrain,
type = "response") #get p predictions
eval <- prediction(log_predict, response) #from ROCR package
auc_calc <- performance(eval,'auc')
auc_calc@y.values #special object; this is how we extract the exact AUC part
library(rpart)
f <- paste('response ~ ',paste(selVars,collapse=' + '),sep='')
system.time(tmodel <- rpart(f,data=dTrain,
control=rpart.control(cp=0.001,minsplit=1000,
minbucket=1000,maxdepth=5)))
dTrain$pred <- predict(tmodel, newdata = dTrain)
dTrain$response <- dTrain$churn > 0
#calculate AUC for CART decision tree
eval <- prediction(dTrain$pred, dTrain$response)
auc_calc <- performance(eval,'auc')
auc_calc@y.values
data(mtcars)
data(mtcars)
head(mtcars)
names(mtcars)
?mtcars
install.packages('class')
library(class)
install.packages("class")
install.packages('class')
library(class)
response <- mtcars$am > 0
system.time(knnDecision <- knn(mtcars,mtcars,response,k=200,prob=T))
?knn #to learn more about the knn implementation
head(knnDecision)
knnPred <- ifelse(knnDecision==TRUE,
attributes(knnDecision)$prob,
1-(attributes(knnDecision)$prob))
head(knnPred)
#calculate AUC
library(ROCR)
eval <- prediction(knnPred, response) #from ROCR package
auc_calc <- performance(eval,'auc')
auc_calc@y.values #special object; this is how we extract the exact AUC part
f <- paste('response ~ ',paste(mtcars[,],collapse=' + '),sep='')
system.time(gmodel <- glm(as.formula(f),
data=mtcars,
family=binomial(link='logit'))) #get system time and train the model
log_predict <- predict(gmodel,
newdata=knnTrain,
type = "response") #get p predictions
system.time(gmodel <- glm(as.formula(f),
data=mtcars,
family=binomial(link='logit'))) #get system time and train the model
log_predict <- predict(gmodel,
newdata=mtcars,
type = "response") #get p predictions
eval <- prediction(log_predict, response) #from ROCR package
auc_calc <- performance(eval,'auc')
auc_calc@y.values #special object; this is how we extract the exact AUC part
