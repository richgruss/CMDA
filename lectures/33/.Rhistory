age <- c(1,2,3,4,5,6,7,8)
weight <- c(5,6,7,2,3,4,9,8)
mean(weight)
mean(age)
cor(age,weight)
plot(age,weight)
weight <- c(12,16,19,32,55,100,200,600)
plot(age,weight)
cor(age,weight)
help.start()
cwd
pwd
#load the iris data
data(iris)
#examine the data
names(iris)
head(iris)
summary(iris)
#load the iris data
data(iris)
#examine the data
names(iris)
head(iris)
summary(iris)
#the last column is the species.  take cols 1-4 for features
features <- iris[,1:4]
#variabels are raw lengths, normalize
scaled_features <- scale(features)
#we need the data as a frame to use Shiny app
scaled_features_frame <-as.data.frame(scaled_features)
#fifth column is species name
label_points <- iris[,5]
#double check data
names(scaled_features_frame)
summary(scaled_features)
#for exploration only--see the distances between
distance <- dist(scaled_features, method = "euclidean")
data(iris)
#examine the data
names(iris)
distance_frame <- as.data.frame(distance)
distance
hier_cl <- hclust(distance, method="ward.D")
plot(hier_cl, labels=iris$Species)
rect.hclust(hier_cl, k=3)
hierarchical_clustering_groups <- cutree(hier_cl, k = 3)
print(hierarchical_clustering_groups)
rect.hclust(hier_cl, k=3)
compare <- cbind(hierarchical_clustering_groups,kmeans_clusters$cluster)
compare <- as.data.frame(compare)
names(compare) <- c("Hierarchical", "kmeans")
compare <- cbind(iris$Species,compare)
compare
kmeans_clusters <- kmeans(distance, 3 , nstart=100, iter.max=100)
kmeans_clusters
compare <- cbind(hierarchical_clustering_groups,kmeans_clusters$cluster)
compare <- as.data.frame(compare)
names(compare) <- c("Hierarchical", "kmeans")
compare <- cbind(iris$Species,compare)
compare
shiny::runApp('git/CMDA/lectures/32/App_3')
#load the iris data
data(iris)
#examine the data
names(iris)
head(iris)
summary(iris)
#the last column is the species.  take cols 1-4 for features
features <- iris[,1:4]
#variabels are raw lengths, normalize
scaled_features <- scale(features)
#FOR 10.3 : we need the data as a frame to use Shiny app
scaled_features_frame <-as.data.frame(scaled_features)
#fifth column is species name
label_points <- iris[,5]
#double check data
names(scaled_features_frame)
summary(scaled_features)
#see the distances between each observed iris
#rows and columns are both iris observations, so this gives
#a symmetrical matrix
distance <- dist(scaled_features, method = "euclidean")
distance
####### 10.1: Hierarchical Clustering #############################
hier_cl <- hclust(distance, method="ward.D")
plot(hier_cl, labels=iris$Species)
rect.hclust(hier_cl, k=3)
hierarchical_clustering_groups <- cutree(hier_cl, k = 3)
#gives a vector that maps each of the 150 observations to a cluster
print(hierarchical_clustering_groups) #print dendrogram
######### 10.2 K-Means Clustering ###################################
kmeans_clusters <- kmeans(distance, 3 , nstart=100, iter.max=100)
kmeans_clusters
#Compare the two clustering algo results
compare <- cbind(hierarchical_clustering_groups,kmeans_clusters$cluster)
compare <- as.data.frame(compare)
names(compare) <- c("Hierarchical", "kmeans")
compare <- cbind(iris$Species,compare)
compare
#both alogorithms do well in grouping the setosas and the versicolors together.
#Hierarchial used cluster 1 and kmeans called it 3, but the important point
#is that both algorithms grouped them accuratey. Both were less successful
#with the virginicas, though.  Both alogorithsm were all over the place
#in the groupings of that species.
shiny::runApp('git/CMDA/lectures/32/App_3')
#Decision trees
setwd('/Users/rgruss/git/CMDA/lectures/33')
load('KDD2009.Rdata')
#Decision trees
setwd('/Users/rgruss/git/CMDA/lectures/33')
load('KDD2009.Rdata')
library(rpart)
head(dTrain)
names(dTrain)#training set
head(dTrain$churn) # 1 and -1 type variable; 1 is pos outcome
dTrain$response <- dTrain$churn > 0
head(dTrain$response)
f <- paste('response ~ ',paste(selVars,collapse=' + '),sep='')
f
tmodel <- rpart(f,data=dTrain,
control=rpart.control(cp=0.001,minsplit=1000,
minbucket=1000,maxdepth=5))
tmodel
?rpart #learn more
?rpart.control #learn.more
#to visualize tree
install.packages("rpart.plot")
library(rpart.plot)
prp(tmodel)
#using the same training data data
#should do for at least one test set as well
dTrain$pred <- predict(tmodel, newdata = dTrain)
eval <- prediction(dTrain$pred, dTrain$response) #from ROCR package
#calculate AUC
auc_calc <- performance(eval,'auc')
#another measure of performance :ROC curve and AUC
install.packages('ROCR') #only need to do this once
library(ROCR)
dTrain$pred <- predict(tmodel, newdata = dTrain)
eval <- prediction(dTrain$pred, dTrain$response) #from ROCR package
auc_calc <- performance(eval,'auc')
auc_calc@y.values #special object; this is how we extract the exact AUC part
